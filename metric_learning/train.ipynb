{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitposembcondafe902e4f67854e6d98f2a8ceedfcf887",
   "display_name": "Python 3.7.6 64-bit ('pos-emb': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('2.1.0', '2.2.4-tf')"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from triplet_preparation import tuples_from_table\n",
    "from model_architecture import triplet_network_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.__version__, tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<KeysViewHDF5 ['tuples_0', 'tuples_1']>\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((72289, 3, 773), (18073, 3, 773), (1,), (1,))"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "'''Test Dataset'''\n",
    "\n",
    "file_path = os.path.abspath('../data/samples/lichess_db_standard_rated_2013-01-tuples.h5')\n",
    "triplets = tuples_from_table(file_path, \"tuples_0\", tuple_indices=[0,1,2])\n",
    "\n",
    "train_triplets, test_triplets = train_test_split(triplets, test_size=0.2, random_state=42)\n",
    "#train_dummy_label = np.zeros_like( (train_triplets.shape[0]),) )\n",
    "#test_dummy_label = np.zeros_like( (test_triplets.shape[0],) )\n",
    "train_triplets.shape, test_triplets.shape #, train_dummy_label.shape, test_dummy_label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.\nModel: \"model_6\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nanchor_input (InputLayer)       [(None, 773)]        0                                            \n__________________________________________________________________________________________________\npositive_input (InputLayer)     [(None, 773)]        0                                            \n__________________________________________________________________________________________________\nnegative_input (InputLayer)     [(None, 773)]        0                                            \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 10)           7740        anchor_input[0][0]               \n                                                                 positive_input[0][0]             \n                                                                 negative_input[0][0]             \n__________________________________________________________________________________________________\ntriplet_loss_layer (TripletLoss ()                   0           dense_6[0][0]                    \n                                                                 dense_6[1][0]                    \n                                                                 dense_6[2][0]                    \n==================================================================================================\nTotal params: 7,740\nTrainable params: 7,740\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "'''Initialize and use triplet network'''\n",
    "input_shape = (773,)\n",
    "embedding_size = 10\n",
    "model = triplet_network_model(input_shape, embedding_size)\n",
    "optimizer = keras.optimizers.Adam(lr = 0.00006)\n",
    "model.compile(loss=None,optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(72289, 773) (72289, 773) (72289, 773)\nTrain on 72289 samples\nEpoch 1/3\n72289/72289 [==============================] - 3s 44us/sample - loss: 10.5650\nEpoch 2/3\n72289/72289 [==============================] - 3s 40us/sample - loss: 7.2362\nEpoch 3/3\n72289/72289 [==============================] - 3s 40us/sample - loss: 5.4560\n\nhistory dict: {'loss': [10.565017690020579, 7.23616203458487, 5.456016604537467]}\n"
    }
   ],
   "source": [
    "anc_train = train_triplets[:,0,:]\n",
    "pos_train = train_triplets[:,1,:]\n",
    "neg_train = train_triplets[:,2,:]\n",
    "\n",
    "anc_test = test_triplets[:,0,:]\n",
    "pos_test = test_triplets[:,1,:]\n",
    "neg_test = test_triplets[:,2,:]\n",
    "\n",
    "print(anc.shape, pos.shape, neg.shape)\n",
    "\n",
    "history = model.fit([anc_train, pos_train, neg_train],\n",
    "                    batch_size=64,\n",
    "                    epochs=3) #,\n",
    "                    #validation_data=([anc_test, pos_test, neg_test]))\n",
    "\n",
    "print('\\nhistory dict:', history.history)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n# Evaluate on test data\n18073/18073 [==============================] - 0s 16us/sample - loss: 9.7712\ntest loss: 9.77121658493525\n"
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate([anc_test, pos_test, neg_test], batch_size=128)\n",
    "print('test loss:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 1000 samples, validate on 1000 samples\nEpoch 1/10\n 960/1000 [===========================>..] - ETA: 0s - loss: 1.2638 - accuracy: 0.6448\nEpoch 00001: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 1s 653us/sample - loss: 1.2335 - accuracy: 0.6530 - val_loss: 0.7204 - val_accuracy: 0.7850\nEpoch 2/10\n 960/1000 [===========================>..] - ETA: 0s - loss: 0.4300 - accuracy: 0.8792\nEpoch 00002: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 296us/sample - loss: 0.4227 - accuracy: 0.8810 - val_loss: 0.5260 - val_accuracy: 0.8370\nEpoch 3/10\n 864/1000 [========================>.....] - ETA: 0s - loss: 0.3052 - accuracy: 0.9167\nEpoch 00003: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 332us/sample - loss: 0.2916 - accuracy: 0.9230 - val_loss: 0.4594 - val_accuracy: 0.8510\nEpoch 4/10\n 992/1000 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9526\nEpoch 00004: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 343us/sample - loss: 0.2001 - accuracy: 0.9520 - val_loss: 0.4495 - val_accuracy: 0.8690\nEpoch 5/10\n 960/1000 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.9573\nEpoch 00005: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 308us/sample - loss: 0.1685 - accuracy: 0.9590 - val_loss: 0.4260 - val_accuracy: 0.8600\nEpoch 6/10\n 800/1000 [=======================>......] - ETA: 0s - loss: 0.1080 - accuracy: 0.9850\nEpoch 00006: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 338us/sample - loss: 0.1146 - accuracy: 0.9830 - val_loss: 0.3896 - val_accuracy: 0.8730\nEpoch 7/10\n 864/1000 [========================>.....] - ETA: 0s - loss: 0.0779 - accuracy: 0.9896\nEpoch 00007: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 304us/sample - loss: 0.0774 - accuracy: 0.9890 - val_loss: 0.4030 - val_accuracy: 0.8740\nEpoch 8/10\n 928/1000 [==========================>...] - ETA: 0s - loss: 0.0647 - accuracy: 0.9914\nEpoch 00008: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 298us/sample - loss: 0.0652 - accuracy: 0.9920 - val_loss: 0.4030 - val_accuracy: 0.8650\nEpoch 9/10\n 800/1000 [=======================>......] - ETA: 0s - loss: 0.0492 - accuracy: 0.9962\nEpoch 00009: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 316us/sample - loss: 0.0472 - accuracy: 0.9970 - val_loss: 0.4128 - val_accuracy: 0.8630\nEpoch 10/10\n 832/1000 [=======================>......] - ETA: 0s - loss: 0.0398 - accuracy: 0.9976\nEpoch 00010: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 306us/sample - loss: 0.0424 - accuracy: 0.9980 - val_loss: 0.3975 - val_accuracy: 0.8780\n"
    }
   ],
   "source": [
    "'''Create Checkpoints during training'''\n",
    "\n",
    "checkpoint_path = \"checkpoints/test_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "history = model.fit(train_images, \n",
    "                    train_labels,  \n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images,test_labels),\n",
    "                    callbacks=[cp_callback])  # Pass callback to training\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1a1a187bd0>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "'''Loads the weights from checkpoint path'''\n",
    "model = create_model()\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/pafrank/anaconda3/envs/pos-emb/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: model/test_model/assets\n"
    }
   ],
   "source": [
    "'''Save the complete model after training'''\n",
    "model.save('model/test_model', save_format='tf') # or save_format='h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "'''Load model from saved state'''\n",
    "model = tf.keras.models.load_model('model/test_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1000/1000 - 0s - loss: 0.3975 - accuracy: 0.8780\nRestored model, accuracy: 87.80%\n(1000, 10)\n"
    }
   ],
   "source": [
    "'''Use loaded model for inference'''\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "print(model.predict(test_images).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}