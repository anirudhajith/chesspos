{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitposembcondafe902e4f67854e6d98f2a8ceedfcf887",
   "display_name": "Python 3.7.6 64-bit ('pos-emb': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('2.1.0', '2.2.4-tf')"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from triplet_preparation import tuples_from_table\n",
    "from model_architecture import triplet_network_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.__version__, tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<KeysViewHDF5 ['tuples_0', 'tuples_1']>\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((72289, 3, 773), (18073, 3, 773))"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "'''Test Dataset'''\n",
    "\n",
    "file_path = os.path.abspath('../data/samples/lichess_db_standard_rated_2013-01-tuples.h5')\n",
    "triplets = tuples_from_table(file_path, \"tuples_0\", tuple_indices=[0,1,6])\n",
    "\n",
    "train_triplets, test_triplets = train_test_split(triplets, test_size=0.2, random_state=42)\n",
    "train_triplets.shape, test_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ad7a2930a427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m773\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_network_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnetwork_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnetwork_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "'''Initialize and use triplet network'''\n",
    "input_shape = ((10000,773))\n",
    "network = triplet_network_model(input_shape, 10)\n",
    "optimizer = keras.optimizers.Adam(lr = 0.00006)\n",
    "network_train.compile(loss=None,optimizer=optimizer)\n",
    "network_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 512)               401920    \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "'''Define Model'''\n",
    "\n",
    "def create_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 1000 samples, validate on 1000 samples\nEpoch 1/10\n 960/1000 [===========================>..] - ETA: 0s - loss: 1.2638 - accuracy: 0.6448\nEpoch 00001: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 1s 653us/sample - loss: 1.2335 - accuracy: 0.6530 - val_loss: 0.7204 - val_accuracy: 0.7850\nEpoch 2/10\n 960/1000 [===========================>..] - ETA: 0s - loss: 0.4300 - accuracy: 0.8792\nEpoch 00002: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 296us/sample - loss: 0.4227 - accuracy: 0.8810 - val_loss: 0.5260 - val_accuracy: 0.8370\nEpoch 3/10\n 864/1000 [========================>.....] - ETA: 0s - loss: 0.3052 - accuracy: 0.9167\nEpoch 00003: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 332us/sample - loss: 0.2916 - accuracy: 0.9230 - val_loss: 0.4594 - val_accuracy: 0.8510\nEpoch 4/10\n 992/1000 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9526\nEpoch 00004: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 343us/sample - loss: 0.2001 - accuracy: 0.9520 - val_loss: 0.4495 - val_accuracy: 0.8690\nEpoch 5/10\n 960/1000 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.9573\nEpoch 00005: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 308us/sample - loss: 0.1685 - accuracy: 0.9590 - val_loss: 0.4260 - val_accuracy: 0.8600\nEpoch 6/10\n 800/1000 [=======================>......] - ETA: 0s - loss: 0.1080 - accuracy: 0.9850\nEpoch 00006: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 338us/sample - loss: 0.1146 - accuracy: 0.9830 - val_loss: 0.3896 - val_accuracy: 0.8730\nEpoch 7/10\n 864/1000 [========================>.....] - ETA: 0s - loss: 0.0779 - accuracy: 0.9896\nEpoch 00007: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 304us/sample - loss: 0.0774 - accuracy: 0.9890 - val_loss: 0.4030 - val_accuracy: 0.8740\nEpoch 8/10\n 928/1000 [==========================>...] - ETA: 0s - loss: 0.0647 - accuracy: 0.9914\nEpoch 00008: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 298us/sample - loss: 0.0652 - accuracy: 0.9920 - val_loss: 0.4030 - val_accuracy: 0.8650\nEpoch 9/10\n 800/1000 [=======================>......] - ETA: 0s - loss: 0.0492 - accuracy: 0.9962\nEpoch 00009: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 316us/sample - loss: 0.0472 - accuracy: 0.9970 - val_loss: 0.4128 - val_accuracy: 0.8630\nEpoch 10/10\n 832/1000 [=======================>......] - ETA: 0s - loss: 0.0398 - accuracy: 0.9976\nEpoch 00010: saving model to checkpoints/test_cp.ckpt\n1000/1000 [==============================] - 0s 306us/sample - loss: 0.0424 - accuracy: 0.9980 - val_loss: 0.3975 - val_accuracy: 0.8780\n"
    }
   ],
   "source": [
    "'''Create Checkpoints during training'''\n",
    "\n",
    "checkpoint_path = \"checkpoints/test_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "history = model.fit(train_images, \n",
    "                    train_labels,  \n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images,test_labels),\n",
    "                    callbacks=[cp_callback])  # Pass callback to training\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1a1a187bd0>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "'''Loads the weights from checkpoint path'''\n",
    "model = create_model()\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/pafrank/anaconda3/envs/pos-emb/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: model/test_model/assets\n"
    }
   ],
   "source": [
    "'''Save the complete model after training'''\n",
    "model.save('model/test_model', save_format='tf') # or save_format='h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "'''Load model from saved state'''\n",
    "model = tf.keras.models.load_model('model/test_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1000/1000 - 0s - loss: 0.3975 - accuracy: 0.8780\nRestored model, accuracy: 87.80%\n(1000, 10)\n"
    }
   ],
   "source": [
    "'''Use loaded model for inference'''\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "print(model.predict(test_images).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}