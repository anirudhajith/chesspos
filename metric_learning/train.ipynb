{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitposembcondafe902e4f67854e6d98f2a8ceedfcf887",
   "display_name": "Python 3.7.6 64-bit ('pos-emb': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('2.1.0', '2.2.4-tf')"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from triplet_preparation import inputs_from_tuples, tuples_from_file_array, train_inputs_file_array_generator, train_inputs_length\n",
    "from model_architecture import triplet_network_model\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.__version__, tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''Test Dataset'''\n",
    "\n",
    "files = [\n",
    "    os.path.abspath('../data/samples/lichess_db_standard_rated_2020-02-06-tuples-strong.h5')\n",
    "]\n",
    "# get triplets\n",
    "triplets = tuples_from_file_array(files, \"tuples\", tuple_indices=[0, 1, 2])\n",
    "# prepare for nn\n",
    "train_triplets, test_triplets = inputs_from_tuples(triplets, test_split=True, test_size=0.05)\n",
    "len(train_triplets),train_triplets[0].shape, len(test_triplets), test_triplets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"embedding_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_layer_0 (Dense)    (None, 42)                32508     \n_________________________________________________________________\nembedding_layer_1 (Dense)    (None, 10)                430       \n=================================================================\nTotal params: 32,938\nTrainable params: 32,938\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.\nModel: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nanchor_input (InputLayer)       [(None, 773)]        0                                            \n__________________________________________________________________________________________________\npositive_input (InputLayer)     [(None, 773)]        0                                            \n__________________________________________________________________________________________________\nnegative_input (InputLayer)     [(None, 773)]        0                                            \n__________________________________________________________________________________________________\nembedding_model (Sequential)    (None, 10)           32938       anchor_input[0][0]               \n                                                                 positive_input[0][0]             \n                                                                 negative_input[0][0]             \n__________________________________________________________________________________________________\ntriplet_loss_layer (TripletLoss ()                   0           embedding_model[1][0]            \n                                                                 embedding_model[2][0]            \n                                                                 embedding_model[3][0]            \n==================================================================================================\nTotal params: 32,938\nTrainable params: 32,938\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "'''Initialize and use triplet network'''\n",
    "input_shape = (773,)\n",
    "embedding_size = 10\n",
    "model = triplet_network_model(input_shape, embedding_size)\n",
    "optimizer = keras.optimizers.Adam(lr = 0.00006)\n",
    "model.compile(loss=None,optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "109626 training samples.\nTrain for 1000 steps\nEpoch 1/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.8839\nEpoch 2/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.5331\nEpoch 3/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.3871\nEpoch 4/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.2964\nEpoch 5/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.2510\nEpoch 6/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.2164\nEpoch 7/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1838\nEpoch 8/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1631\nEpoch 9/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1591\nEpoch 10/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1493\nEpoch 11/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1205\nEpoch 12/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1079\nEpoch 13/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1128\nEpoch 14/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1191\nEpoch 15/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1038\nEpoch 16/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.1004\nEpoch 17/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0931\nEpoch 18/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0941\nEpoch 19/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0913\nEpoch 20/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0758\nEpoch 21/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0732\nEpoch 22/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0787\nEpoch 23/27\n1000/1000 [==============================] - 6s 6ms/step - loss: 0.0804\nEpoch 24/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0706\nEpoch 25/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0715\nEpoch 26/27\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.0646\nEpoch 27/27\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.0664\n"
    }
   ],
   "source": [
    "files = [\n",
    "    #os.path.abspath('../data/samples/lichess_db_standard_rated_2020-02-06-tuples-strong.h5')\n",
    "    os.path.abspath('../data/samples/lichess_db_standard_rated_2013-01-tuples.h5')\n",
    "]\n",
    "\n",
    "batch_size = 16\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "\n",
    "train_len = train_inputs_length(files, table_id_prefix=\"tuples\")\n",
    "print(f\"{train_len} training samples.\")\n",
    "\n",
    "train_generator = train_inputs_file_array_generator(files, table_id_prefix=\"tuples\",\n",
    "\t\t\t\t\ttuple_indices=[0,1,2,3,4,5,6], batch_size=batch_size)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=int(4*train_len/steps_per_epoch/batch_size))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_triplets,\n",
    "                    batch_size=64,\n",
    "                    epochs=3) #,\n",
    "                    #validation_data=([anc_test, pos_test, neg_test]))\n",
    "\n",
    "print('\\nhistory dict:', history.history)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate([anc_test, pos_test, neg_test], batch_size=128)\n",
    "print('test loss:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create Checkpoints during training'''\n",
    "\n",
    "checkpoint_path = \"checkpoints/test_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "history = model.fit(train_images, \n",
    "                    train_labels,  \n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images,test_labels),\n",
    "                    callbacks=[cp_callback])  # Pass callback to training\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loads the weights from checkpoint path'''\n",
    "model = create_model()\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the complete model after training'''\n",
    "model.save('model/test_model', save_format='tf') # or save_format='h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load model from saved state'''\n",
    "model = tf.keras.models.load_model('model/test_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use loaded model for inference'''\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "print(model.predict(test_images).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}